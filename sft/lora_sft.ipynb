{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408e4b71-7896-4bb7-a22f-169055950115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T06:57:26.838900Z",
     "iopub.status.busy": "2024-11-21T06:57:26.838554Z",
     "iopub.status.idle": "2024-11-21T06:57:31.859233Z",
     "shell.execute_reply": "2024-11-21T06:57:31.858610Z",
     "shell.execute_reply.started": "2024-11-21T06:57:26.838875Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d3b4a-43af-43ec-afd1-545e15febfe8",
   "metadata": {},
   "source": [
    "# 加载1.8B Chat Int4模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b962d5-3b98-408b-b132-2d5e73e6fa23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T06:59:51.023210Z",
     "iopub.status.busy": "2024-11-21T06:59:51.022548Z",
     "iopub.status.idle": "2024-11-21T06:59:53.916473Z",
     "shell.execute_reply": "2024-11-21T06:59:53.915926Z",
     "shell.execute_reply.started": "2024-11-21T06:59:51.023180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:59:51,450 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Some weights of the model checkpoint at /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.9.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen-1_8B-Chat-Int4')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5755a33-5de8-4729-822c-5666bdb6f1c1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T07:09:43.441426Z",
     "iopub.status.busy": "2024-11-28T07:09:43.441102Z",
     "iopub.status.idle": "2024-11-28T07:10:09.734065Z",
     "shell.execute_reply": "2024-11-28T07:10:09.733625Z",
     "shell.execute_reply.started": "2024-11-28T07:09:43.441407Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 15:09:43,797 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94eab51068ea4479a1f3bba9f5292f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "# model_dir = snapshot_download('qwen/Qwen-7B')\n",
    "# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n",
    "# model_dir = snapshot_download('qwen/Qwen-14B')\n",
    "model_dir = snapshot_download('qwen/Qwen-1_8B-Chat')\n",
    "\n",
    "# Loading local checkpoints\n",
    "# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe5ed3c-e0ee-4577-8962-c69a3f1917f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:10:47.657184Z",
     "iopub.status.busy": "2024-11-28T07:10:47.656856Z",
     "iopub.status.idle": "2024-11-28T07:10:53.952417Z",
     "shell.execute_reply": "2024-11-28T07:10:53.951871Z",
     "shell.execute_reply.started": "2024-11-28T07:10:47.657164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 15:10:48,407 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n",
      "浙江省的省会是杭州。\n",
      "杭州有许多有趣的地方，比如西湖、灵隐寺、宋城等。此外，杭州还有许多美丽的自然景观和历史遗迹。\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"qwen/Qwen-1_8B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020af6b3-022e-4cad-967c-e8144e71780a",
   "metadata": {},
   "source": [
    "# 提示词工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbb0900-6554-4751-bbc8-326fac486cbb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T08:56:16.393982Z",
     "iopub.status.busy": "2024-11-28T08:56:16.393659Z",
     "iopub.status.idle": "2024-11-28T08:56:16.399496Z",
     "shell.execute_reply": "2024-11-28T08:56:16.398977Z",
     "shell.execute_reply.started": "2024-11-28T08:56:16.393961Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 城市数据\n",
    "with open('/mnt/workspace/city.txt','r',encoding='utf-8') as fp:\n",
    "    city_list=fp.readlines()\n",
    "    city_list=[line.strip().split(' ')[1] for line in city_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6338a20-6c9c-49fa-9f5b-9c606d747260",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T08:56:15.046938Z",
     "iopub.status.busy": "2024-11-28T08:56:15.046587Z",
     "iopub.status.idle": "2024-11-28T08:56:15.050729Z",
     "shell.execute_reply": "2024-11-28T08:56:15.050197Z",
     "shell.execute_reply.started": "2024-11-28T08:56:15.046916Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q='青岛4月6日下雨么?'\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65138cce-9cd6-4dcf-8d61-025a13864f6c",
   "metadata": {},
   "source": [
    "# 生成SFT微调数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d76f5-0008-42a1-a459-0d9d54d23ce6",
   "metadata": {},
   "source": [
    "Qwen的SFT数据格式要求:\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你好\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"我是一个语言模型，我叫通义千问。\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9673d1-3359-41ef-b851-a4be82765701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T07:01:22.999755Z",
     "iopub.status.busy": "2024-11-21T07:01:22.999390Z",
     "iopub.status.idle": "2024-11-21T07:01:23.358602Z",
     "shell.execute_reply": "2024-11-21T07:01:23.358005Z",
     "shell.execute_reply.started": "2024-11-21T07:01:22.999730Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time \n",
    "\n",
    "Q_arr=[]\n",
    "A_arr=[]\n",
    "\n",
    "Q_list=[\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？','%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？','%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？','%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？','%m-%d'),\n",
    "]\n",
    "\n",
    "# 生成一批\"1月2号\"、\"1月2日\"、\"2023年1月2号\", \"2023年1月2日\", \"2023-02-02\", \"03-02\"之类的话术, 教会它做日期转换\n",
    "for i in range(1000):\n",
    "    Q=Q_list[random.randint(0,len(Q_list)-1)]\n",
    "    city=city_list[random.randint(0,len(city_list)-1)]\n",
    "    year=random.randint(1990,2025)\n",
    "    month=random.randint(1,12)\n",
    "    day=random.randint(1,28)\n",
    "    time_str='{}-{}-{}'.format(year,month,day)\n",
    "    date_field=time.strftime(Q[1],time.strptime(time_str,'%Y-%m-%d'))\n",
    "    Q=Q[0].format(city=city,year=year,month=month,day=day) # 问题\n",
    "    A=json.dumps({'city':city,'date':date_field},ensure_ascii=False)  # 回答\n",
    "\n",
    "    Q_arr.append(prompt_template%(Q,))\n",
    "    A_arr.append(A)\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df=pd.DataFrame({'Prompt':Q_arr,'Completion':A_arr})\n",
    "df.to_excel('train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5c456ab-e44d-49cc-95bb-8c3fd21411d2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T08:56:32.369170Z",
     "iopub.status.busy": "2024-11-28T08:56:32.368843Z",
     "iopub.status.idle": "2024-11-28T08:56:32.469037Z",
     "shell.execute_reply": "2024-11-28T08:56:32.468507Z",
     "shell.execute_reply.started": "2024-11-28T08:56:32.369151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 问句模板列表\n",
    "Q_list = [\n",
    "    ('{city}{year}年{month}月{day}日的天气', '%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气', '%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气', '%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气', '%m-%d'),\n",
    "    ('{year}年{month}月{day}日{city}的天气', '%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气', '%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气', '%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气', '%m-%d'),\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？', '%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？', '%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？', '%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？', '%m-%d'),\n",
    "]\n",
    "\n",
    "# 初始化问题和答案列表\n",
    "Q_arr = []\n",
    "A_arr = []\n",
    "train_data = []\n",
    "\n",
    "# 生成数据\n",
    "for i in range(1000):\n",
    "    Q = Q_list[random.randint(0, len(Q_list) - 1)]\n",
    "    city = city_list[random.randint(0, len(city_list) - 1)]\n",
    "    year = random.randint(1990, 2025)\n",
    "    month = random.randint(1, 12)\n",
    "    day = random.randint(1, 28)\n",
    "    \n",
    "    time_str = '{}-{}-{}'.format(year, month, day)\n",
    "    date_field = time.strftime(Q[1], time.strptime(time_str, '%Y-%m-%d'))\n",
    "    \n",
    "    # 生成问句和答案\n",
    "    Q_text = Q[0].format(city=city, year=year, month=month, day=day)\n",
    "    A = json.dumps({'city': city, 'date': date_field}, ensure_ascii=False)\n",
    "    \n",
    "    # 填充数据\n",
    "    Q_arr.append(prompt_template % (Q_text,))\n",
    "    A_arr.append(A)\n",
    "    \n",
    "    # 构建训练数据样本\n",
    "    example = {\n",
    "        'id': 'identity_{}'.format(i),\n",
    "        'conversations': [\n",
    "            {'from': 'user', 'value': prompt_template % (Q_text,)},\n",
    "            {'from': 'assistant', 'value': A}\n",
    "        ]\n",
    "    }\n",
    "    train_data.append(example)\n",
    "\n",
    "# 保存为Excel文件\n",
    "df = pd.DataFrame({'Prompt': Q_arr, 'Completion': A_arr})\n",
    "df.to_excel('train.xlsx', index=False)\n",
    "\n",
    "# 保存为JSON格式\n",
    "with open('train.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(json.dumps(train_data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c079eb9-1fc1-42dc-8fe3-e263b6686ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T07:47:32.696293Z",
     "iopub.status.busy": "2024-11-21T07:47:32.695874Z",
     "iopub.status.idle": "2024-11-21T07:47:36.843144Z",
     "shell.execute_reply": "2024-11-21T07:47:36.842508Z",
     "shell.execute_reply.started": "2024-11-21T07:47:32.696259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'Qwen'...\n",
      "remote: Enumerating objects: 1763, done.\u001b[K\n",
      "remote: Counting objects: 100% (837/837), done.\u001b[K\n",
      "remote: Compressing objects: 100% (327/327), done.\u001b[K\n",
      "remote: Total 1763 (delta 625), reused 594 (delta 495), pack-reused 926 (from 1)\u001b[K\n",
      "接收对象中: 100% (1763/1763), 35.94 MiB | 16.65 MiB/s, 完成.\n",
      "处理 delta 中: 100% (1039/1039), 完成.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/QwenLM/Qwen.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df864230-32ac-4e01-ab5a-83a6758c773f",
   "metadata": {},
   "source": [
    "# 微调模型，生成到output_qwen\n",
    "\n",
    "bash finetune/finetune_qlora_single_gpu.sh  -m /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 -d /root/Qwen/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a5cf98-3325-4f8f-8058-1d4a24ec83db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:19.488146Z",
     "iopub.status.busy": "2024-11-28T07:22:19.487806Z",
     "iopub.status.idle": "2024-11-28T07:22:19.761243Z",
     "shell.execute_reply": "2024-11-28T07:22:19.760674Z",
     "shell.execute_reply.started": "2024-11-28T07:22:19.488126Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/mnt/workspace/finetune.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash /mnt/workspace/Qwen/finetune/finetune_qlora_single_gpu.sh -m /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat -d /mnt/workspace/Qwen/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86127073-2095-4648-b85d-ca16b24eb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash /mnt/workspace/Qwen/finetune/finetune_qlora_single_gpu.sh -m /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat -d /mnt/workspace/Qwen/train.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b107b-4c01-4853-bb51-ca1552ab3314",
   "metadata": {},
   "source": [
    "# 加载SFT后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a4a4f0-32ef-4254-a3f1-31648e580fa6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T10:21:17.964368Z",
     "iopub.status.busy": "2024-11-28T10:21:17.964011Z",
     "iopub.status.idle": "2024-11-28T10:21:20.464289Z",
     "shell.execute_reply": "2024-11-28T10:21:20.463784Z",
     "shell.execute_reply.started": "2024-11-28T10:21:17.964346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1712f55905f74ac585a855394ebdc677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    '/mnt/workspace/Qwen/output_qwen', # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a053c4a-d8b1-44e9-b4f0-0906f9115333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:21:23.958959Z",
     "iopub.status.busy": "2024-11-28T10:21:23.958634Z",
     "iopub.status.idle": "2024-11-28T10:21:29.849822Z",
     "shell.execute_reply": "2024-11-28T10:21:29.849349Z",
     "shell.execute_reply.started": "2024-11-28T10:21:23.958938Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:2020年4月16号三亚下雨么？\n",
      "A:{\"city\": \"三亚\", \"date\": \"2020-04-16\"}\n",
      "\n",
      "Q:青岛3-15号天气预报\n",
      "A:{\"city\": \"青岛\", \"date\": \"03-15\"}\n",
      "\n",
      "Q:5月6号下雪么，城市是威海\n",
      "A:{\"city\": \"威海\", \"date\": \"05-06\"}\n",
      "\n",
      "Q:青岛2023年12月30号有雾霾么?\n",
      "A:{\"city\": \"青岛\", \"date\": \"2023-12-30\"}\n",
      "\n",
      "Q:我打算6月1号去北京旅游，请问天气怎么样？\n",
      "A:{\"city\": \"北京\", \"date\": \"06-01\"}\n",
      "\n",
      "Q:你们打算1月3号坐哪一趟航班去上海？\n",
      "A:{\"city\": \"上海\", \"date\": \"01-03\"}\n",
      "\n",
      "Q:小明和小红是8月8号在上海结婚么?\n",
      "A:{\"city\": \"上海\", \"date\": \"08-08\"}\n",
      "\n",
      "Q:一起去东北看冰雕么，大概是1月15号左右，我们3个人一起\n",
      "A:{\"city\": \"东北\", \"date\": \"01-15\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.top_p=0 # 只选择概率最高的token\n",
    "\n",
    "Q_list=['2020年4月16号三亚下雨么？','青岛3-15号天气预报','5月6号下雪么，城市是威海','青岛2023年12月30号有雾霾么?','我打算6月1号去北京旅游，请问天气怎么样？','你们打算1月3号坐哪一趟航班去上海？','小明和小红是8月8号在上海结婚么?',\n",
    "        '一起去东北看冰雕么，大概是1月15号左右，我们3个人一起']\n",
    "for Q in Q_list:\n",
    "    prompt=prompt_template%(Q,)\n",
    "    A,hist=model.chat(tokenizer,prompt,history=None)\n",
    "    print('Q:%s\\nA:%s\\n'%(Q,A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39e860c1-4572-456d-aedc-703f192f599a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:21:35.365742Z",
     "iopub.status.busy": "2024-11-28T10:21:35.365382Z",
     "iopub.status.idle": "2024-11-28T10:21:37.705617Z",
     "shell.execute_reply": "2024-11-28T10:21:37.705154Z",
     "shell.execute_reply.started": "2024-11-28T10:21:35.365721Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 钓鱼时要选择合适的地点，最好是在有遮挡物的地方，避免阳光直射。\n",
      "2. 钓鱼时要注意天气变化，及时调整钓鱼姿势和时间。\n",
      "3. 钓鱼时要保持耐心，不要急于求成。\n"
     ]
    }
   ],
   "source": [
    "prompt='青岛海边钓鱼需要特别注意什么？'\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d17936-2dc6-4942-9199-f619960b1ee3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ce7aa6-383b-4e35-9e25-ff05b29f6f71",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T10:21:42.554003Z",
     "iopub.status.busy": "2024-11-28T10:21:42.553678Z",
     "iopub.status.idle": "2024-11-28T10:21:44.509598Z",
     "shell.execute_reply": "2024-11-28T10:21:44.509120Z",
     "shell.execute_reply.started": "2024-11-28T10:21:42.553984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东京是日本的首都，也是该国的政治、经济和文化中心。它位于本州岛中部，东临太平洋，西隔海峡与九州岛相望；气候属于温带海洋性季风气候，四季分明。\n"
     ]
    }
   ],
   "source": [
    "prompt='东京会什么？'\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab8a1e-1626-41c0-845a-38009a9d4da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
